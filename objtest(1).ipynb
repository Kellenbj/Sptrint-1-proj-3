{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already up-to-date: tensorflow==2.2.0 in ./.local/lib/python3.8/site-packages (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in ./.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in ./.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in ./.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (2.2.2)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.3.0,>=2.2.0 in ./.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in ./.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in ./.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in ./.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.3.3 in ./.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in ./.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.2.0) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/lib/python3/dist-packages (from tensorflow==2.2.0) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in ./.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in ./.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (3.13.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in ./.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in ./.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in ./.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.32.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in ./.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (45.2.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in ./.local/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in ./.local/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.22.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in ./.local/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in ./.local/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in ./.local/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in ./.local/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in ./.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in ./.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.5\" in ./.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in ./.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in ./.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in ./.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U --pre tensorflow==\"2.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'models'...\n",
      "remote: Enumerating objects: 2234, done.\u001b[K\n",
      "remote: Counting objects: 100% (2234/2234), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1930/1930), done.\u001b[K\n",
      "remote: Total 2234 (delta 541), reused 939 (delta 279), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (2234/2234), 30.48 MiB | 1.91 MiB/s, done.\n",
      "Resolving deltas: 100% (541/541), done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "  \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "  Puts image into numpy array to feed into tensorflow graph.\n",
    "  Note that by convention we put it into a numpy array with shape\n",
    "  (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "  Args:\n",
    "    path: the file path to the image\n",
    "\n",
    "  Returns:\n",
    "    uint8 numpy array with shape (img_height, img_width, 3)\n",
    "  \"\"\"\n",
    "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "  image = Image.open(BytesIO(img_data))\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "def get_keypoint_tuples(eval_config):\n",
    "  \"\"\"Return a tuple list of keypoint edges from the eval config.\n",
    "  \n",
    "  Args:\n",
    "    eval_config: an eval config containing the keypoint edges\n",
    "  \n",
    "  Returns:\n",
    "    a list of edge tuples, each in the format (start, end)\n",
    "  \"\"\"\n",
    "  tuple_list = []\n",
    "  kp_list = eval_config.keypoint_edge\n",
    "  for edge in kp_list:\n",
    "    tuple_list.append((edge.start, edge.end))\n",
    "  return tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Choose the model to use, then evaluate the cell.\n",
    "MODELS = {'centernet_with_keypoints': 'centernet_hg104_512x512_coco17_tpu-32', 'centernet_without_keypoints': 'centernet_hg104_512x512_coco17_tpu-8'}\n",
    "\n",
    "model_display_name = 'centernet_with_keypoints' # @param ['centernet_with_keypoints', 'centernet_without_keypoints']\n",
    "model_name = MODELS[model_display_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-10-11 17:57:03--  http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_hg104_512x512_kpts_coco17_tpu-32.tar.gz\n",
      "Resolving download.tensorflow.org (download.tensorflow.org)... 2607:f8b0:400e:c07::80, 74.125.20.128\n",
      "Connecting to download.tensorflow.org (download.tensorflow.org)|2607:f8b0:400e:c07::80|:80... failed: Connection refused.\n",
      "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.20.128|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1453232297 (1.4G) [application/x-tar]\n",
      "Saving to: ‘centernet_hg104_512x512_kpts_coco17_tpu-32.tar.gz’\n",
      "\n",
      "centernet_hg104_512 100%[===================>]   1.35G  18.7MB/s    in 84s     \n",
      "\n",
      "2020-10-11 17:58:29 (16.5 MB/s) - ‘centernet_hg104_512x512_kpts_coco17_tpu-32.tar.gz’ saved [1453232297/1453232297]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the checkpoint and put it into models/research/object_detection/test_data/\n",
    "\n",
    "if model_display_name == 'centernet_with_keypoints':\n",
    "  !wget http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_hg104_512x512_kpts_coco17_tpu-32.tar.gz\n",
    "  !tar -xf centernet_hg104_512x512_kpts_coco17_tpu-32.tar.gz\n",
    "  !mv centernet_hg104_512x512_kpts_coco17_tpu-32/checkpoint models/research/object_detection/test_data/\n",
    "else:\n",
    "  !wget http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_hg104_512x512_coco17_tpu-8.tar.gz\n",
    "  !tar -xf centernet_hg104_512x512_coco17_tpu-8.tar.gz\n",
    "  !mv centernet_hg104_512x512_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "models/research/object_detection/configs/tf2/centernet_hg104_512x512_coco17_tpu-32.config; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d69de3b57d22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load pipeline config and build a detection model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mconfigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_configs_from_pipeline_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m detection_model = model_builder.build(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/object_detection/utils/config_util.py\u001b[0m in \u001b[0;36mget_configs_from_pipeline_file\u001b[0;34m(pipeline_config_path, config_override)\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0mpipeline_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainEvalPipelineConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_config_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mproto_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0mtext_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconfig_override\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    114\u001b[0m       \u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mregular\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \"\"\"\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         raise errors.PermissionDeniedError(None, None,\n\u001b[1;32m     77\u001b[0m                                            \"File isn't open for reading\")\n\u001b[0;32m---> 78\u001b[0;31m       self._read_buf = _pywrap_file_io.BufferedInputStream(\n\u001b[0m\u001b[1;32m     79\u001b[0m           self.__name, 1024 * 512)\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: models/research/object_detection/configs/tf2/centernet_hg104_512x512_coco17_tpu-32.config; No such file or directory"
     ]
    }
   ],
   "source": [
    "pipeline_config = os.path.join('models/research/object_detection/configs/tf2/',\n",
    "                                model_name + '.config')\n",
    "model_dir = 'models/research/object_detection/test_data/checkpoint/'\n",
    "\n",
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(\n",
    "      model_config=model_config, is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(\n",
    "      model=detection_model)\n",
    "ckpt.restore(os.path.join(model_dir, 'ckpt-0')).expect_partial()\n",
    "\n",
    "def get_model_detection_function(model):\n",
    "  \"\"\"Get a tf.function for detection.\"\"\"\n",
    "\n",
    "  @tf.function\n",
    "  def detect_fn(image):\n",
    "    \"\"\"Detect objects in image.\"\"\"\n",
    "\n",
    "    image, shapes = model.preprocess(image)\n",
    "    prediction_dict = model.predict(image, shapes)\n",
    "    detections = model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "    return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
    "\n",
    "  return detect_fn\n",
    "\n",
    "detect_fn = get_model_detection_function(detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "PATH_TO_BE_CONFIGURED/label_map.txt; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-d346cf84c2c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabel_map_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval_input_config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_map_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_map_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_labelmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m categories = label_map_util.convert_label_map_to_categories(\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabel_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_num_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_map_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_label_map_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/object_detection/utils/label_map_util.py\u001b[0m in \u001b[0;36mload_labelmap\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    154\u001b[0m   \"\"\"\n\u001b[1;32m    155\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mlabel_map_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring_int_label_map_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIntLabelMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    114\u001b[0m       \u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mregular\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \"\"\"\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         raise errors.PermissionDeniedError(None, None,\n\u001b[1;32m     77\u001b[0m                                            \"File isn't open for reading\")\n\u001b[0;32m---> 78\u001b[0;31m       self._read_buf = _pywrap_file_io.BufferedInputStream(\n\u001b[0m\u001b[1;32m     79\u001b[0m           self.__name, 1024 * 512)\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: PATH_TO_BE_CONFIGURED/label_map.txt; No such file or directory"
     ]
    }
   ],
   "source": [
    "label_map_path = configs['eval_input_config'].label_map_path\n",
    "label_map = label_map_util.load_labelmap(label_map_path)\n",
    "categories = label_map_util.convert_label_map_to_categories(\n",
    "    label_map,\n",
    "    max_num_classes=label_map_util.get_max_label_map_index(label_map),\n",
    "    use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "label_map_dict = label_map_util.get_label_map_dict(label_map, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detect_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-771f7fef8fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m input_tensor = tf.convert_to_tensor(\n\u001b[1;32m     14\u001b[0m     np.expand_dims(image_np, 0), dtype=tf.float32)\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mlabel_id_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'detect_fn' is not defined"
     ]
    }
   ],
   "source": [
    "image_dir = 'models/research/object_detection/test_images/'\n",
    "image_path = os.path.join(image_dir, 'image2.jpg')\n",
    "image_np = load_image_into_numpy_array(image_path)\n",
    "\n",
    "# Things to try:\n",
    "# Flip horizontally\n",
    "# image_np = np.fliplr(image_np).copy()\n",
    "\n",
    "# Convert image to grayscale\n",
    "# image_np = np.tile(\n",
    "#     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
    "\n",
    "input_tensor = tf.convert_to_tensor(\n",
    "    np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "\n",
    "label_id_offset = 1\n",
    "image_np_with_detections = image_np.copy()\n",
    "\n",
    "# Use keypoints if available in detections\n",
    "keypoints, keypoint_scores = None, None\n",
    "if 'detection_keypoints' in detections:\n",
    "  keypoints = detections['detection_keypoints'][0].numpy()\n",
    "  keypoint_scores = detections['detection_keypoint_scores'][0].numpy()\n",
    "\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np_with_detections,\n",
    "      detections['detection_boxes'][0].numpy(),\n",
    "      (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
    "      detections['detection_scores'][0].numpy(),\n",
    "      category_index,\n",
    "      use_normalized_coordinates=True,\n",
    "      max_boxes_to_draw=200,\n",
    "      min_score_thresh=.30,\n",
    "      agnostic_mode=False,\n",
    "      keypoints=keypoints,\n",
    "      keypoint_scores=keypoint_scores,\n",
    "      keypoint_edges=get_keypoint_tuples(configs['eval_config']))\n",
    "\n",
    "plt.figure(figsize=(12,16))\n",
    "plt.imshow(image_np_with_detections)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
